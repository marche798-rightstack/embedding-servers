import asyncio
import base64
import io
from contextlib import asynccontextmanager
from typing import List, Optional, Union

import httpx
import torch
import uvicorn
from fastapi import FastAPI, HTTPException
from PIL import Image
from pydantic import BaseModel, Field
from transformers import AutoImageProcessor, AutoModel

# --- ëª¨ë¸ ë° ì¥ì¹˜ ì„¤ì • ---
MODEL_NAME = "facebook/dinov2-large"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
    print(f"ğŸš€ Loading {MODEL_NAME} on {DEVICE}...")
    app.state.processor = AutoImageProcessor.from_pretrained(MODEL_NAME)
    app.state.model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)
    app.state.model.eval()
    app.state.http_client = httpx.AsyncClient()
    yield
    # ìì› í•´ì œ
    await app.state.http_client.aclose()

app = FastAPI(lifespan=lifespan)

# --- ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ (OpenAI ê·œê²© ìµœì í™”) ---
class EmbeddingRequest(BaseModel):
    input: Union[str, List[str]]
    model: Optional[str] = "dinov2-large"
    normalize: Optional[bool] = True
    # ì¶”ê°€: OpenAI ê·œê²©ì— ìˆëŠ” í•„ë“œ
    user: Optional[str] = None

# --- ì´ë¯¸ì§€ ë¡œë” (ë¹„ë™ê¸° ë° ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”) ---
async def fetch_image(client: httpx.AsyncClient, source: str) -> Image.Image:
    try:
        if source.startswith(('http://', 'https://')):
            resp = await client.get(source, timeout=10.0)
            resp.raise_for_status()
            content = resp.content
        elif source.startswith('data:image'):
            content = base64.b64decode(source.split(',', 1)[1])
        else:
            # ìˆœìˆ˜ base64 ì²˜ë¦¬
            content = base64.b64decode(source)
        
        return Image.open(io.BytesIO(content)).convert('RGB')
    except Exception as e:
        raise ValueError(f"Image load error ({source[:20]}...): {str(e)}")

# --- ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ ---
@app.post("/v1/embeddings")
async def create_embeddings(request: EmbeddingRequest):
    inputs_raw = [request.input] if isinstance(request.input, str) else request.input
    
    if not inputs_raw:
        raise HTTPException(status_code=400, detail="Empty input")

    try:
        # 1. ë¹„ë™ê¸° ë³‘ë ¬ ì´ë¯¸ì§€ ë¡œë“œ
        image_tasks = [fetch_image(app.state.http_client, src) for src in inputs_raw]
        images = await asyncio.gather(*image_tasks)
        
        # 2. ë°°ì¹˜ ì „ì²˜ë¦¬
        inputs = app.state.processor(images=images, return_tensors="pt").to(DEVICE)
        
        # 3. ëª¨ë¸ ì¶”ë¡  (ë°°ì¹˜ ì²˜ë¦¬)
        with torch.no_grad():
            outputs = app.state.model(**inputs)
            # CLS í† í° ì‚¬ìš© (DINOv2 í‘œì¤€)
            embeddings = outputs.last_hidden_state[:, 0, :]
            
            if request.normalize:
                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
        
        # 4. ê²°ê³¼ í¬ë§·íŒ… (OpenAI ê·œê²© ì¤€ìˆ˜)
        embeddings_list = embeddings.cpu().numpy().tolist()
        data = [
            {
                "object": "embedding",
                "index": i,
                "embedding": emb
            }
            for i, emb in enumerate(embeddings_list)
        ]
        
        return {
            "object": "list",
            "data": data,
            "model": request.model,
            "usage": {
                "prompt_tokens": len(inputs_raw), # ì´ë¯¸ì§€ë‹¹ 1í† í°ìœ¼ë¡œ ê³„ì‚°í•˜ê±°ë‚˜ íŒ¨ì¹˜ ìˆ˜ ì ìš©
                "total_tokens": len(inputs_raw)
            }
        }

    except ValueError as ve:
        # OpenAI ìŠ¤íƒ€ì¼ ì—ëŸ¬ ì‘ë‹µ
        return {"error": {"message": str(ve), "type": "invalid_request_error"}}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal Server Error: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8012)